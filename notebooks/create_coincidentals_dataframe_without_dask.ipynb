{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fitsio\n",
    "from pathlib import Path, PurePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coincidentals(spikes_list, idx):\n",
    "    \n",
    "    # Spikes coordinates at given wavelength index\n",
    "    spikes_w = spikes_list[idx]\n",
    "    # Associated neighbour coordinates\n",
    "    nb_pixels = index_8nb[spikes_w[0, :], :]\n",
    "    # Sublist of spikes data that will excludes the one serving as template\n",
    "    spikes_sublist = spikes_list[:idx]+spikes_list[idx+1:]\n",
    "    # Coincidental cross-referencing. \n",
    "    mask_w_arr = np.array([np.isin(nb_pixels, index_8nb[spikes[0,:], :]).any(axis=1) for spikes in spikes_sublist])\n",
    "    select_pixels = mask_w_arr.any(axis=0)\n",
    "    coords_w = spikes_w[0, select_pixels] \n",
    "    w_tables = np.insert(mask_w_arr[:, select_pixels], idx, True, axis=0)\n",
    "    # Retrieve intensity values for the selected coordinates\n",
    "    intensities = spikes_w[ 1:, select_pixels]\n",
    "    arr_w = np.concatenate([coords_w[np.newaxis,...], intensities, w_tables], axis=0)\n",
    "    arr_w = np.insert(arr_w, 3, idx, axis=0)\n",
    "    \n",
    "    return arr_w\n",
    "\n",
    "\n",
    "def process_group(group_n):\n",
    "    fpaths = path_Series.loc[group_n]\n",
    "    spikes_list = [fitsio.read(os.path.join(data_dir, f)) for f in fpaths]\n",
    "    group_data = np.concatenate([extract_coincidentals(spikes_list, i) for i in range(7)], axis=1)\n",
    "    column_names = ['coords' , 'int1', 'int2', 'wref', 'w0', 'w1', 'w2', 'w3', 'w4', 'w5', 'w6']\n",
    "    coincidental_spikes_df = pd.DataFrame(group_data.T, columns=column_names)\n",
    "    coincidental_spikes_df['GroupNumber'] = group_n\n",
    "    return coincidental_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = os.path.join(os.environ['SPIKESDATA'], 'parquet_dataframes')\n",
    "outputdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.environ['SPIKESDATA']\n",
    "spikes_df = pd.read_parquet(os.path.join(data_dir, 'spikes_df_2010_filtered.parquet'), engine='pyarrow')\n",
    "spikes_df2 = spikes_df.set_index(['GroupNumber', 'Time'])\n",
    "path_Series = spikes_df2['Path']\n",
    "path_Series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tintervals = pd.interval_range(start=pd.Timestamp('2010-05-13 00:00:00', tz='UTC'), end=spikes_df['Time'].iloc[-1], freq='D', closed='left')\n",
    "tintervals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Pre-compute the 8-connectivity lookup table. This will be shared across parallel workers.\n",
    "################################################################################################\n",
    "# List of relative 2D coordinates for 8-neighbour connectiviy (9-element list). 1st one is the origin pixel.\n",
    "coords_8nb = np.array([[0, 0], [-1, 0], [-1, -1], [0, -1], [1, -1], [1, 0], [1, 1], [0, 1], [-1, 1]])\n",
    "# Array of 2D coordinates for a 4096 x 4096 array. Matrix convention is kept. [rows, cols] = [y-axis, x-axis]\n",
    "ny, nx = [4096, 4096]\n",
    "coords_1d = np.arange(nx * ny)\n",
    "coordy, coordx = np.unravel_index(coords_1d, [ny, nx]) # also possible by raveling a meshgrid() output\n",
    "coords2d = np.array([coordy, coordx])\n",
    "# Create the array of 2D coordinates of 8-neighbours associated with each pixel.\n",
    "# pixel 0 has 8 neighbour + itself, pixel 1 has 8 neighbour + itself, etc...\n",
    "coords2d_8nb = coords2d[np.newaxis, ...] + coords_8nb[..., np.newaxis]\n",
    "# Handle off-edges coordinates by clipping to the edges, operation done in-place. Here, square detector assumed. Update\n",
    "# to per-axis clipping if that ever changes for another instrument.\n",
    "np.clip(coords2d_8nb, 0, nx-1, out=coords2d_8nb)\n",
    "# Convert to 1D coordinates.\n",
    "index_8nb = np.array([coords2d_8nb[i, 0, :] * nx + coords2d_8nb[i, 1, :] for i in range(len(coords_8nb))],\n",
    "                     dtype='int32', order='C').T\n",
    "index_8nb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups= spikes_df['GroupNumber'].loc[(spikes_df['Time'] >= tintervals[0].left) &  (spikes_df['Time'] < tintervals[0].right)].unique()\n",
    "# len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups= spikes_df['GroupNumber'].loc[(spikes_df['Time'] >= '2010-12-01 00:00:00') &  (spikes_df['Time'] < '2010-12-01 23:59:59')].unique()\n",
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# with Pool(processes=8) as pool:\n",
    "#     # loop over smaller chunks and write to parquet file\n",
    "#     group_df_list = pool.map(process_group, groups)\n",
    "#     df = pd.concat(group_df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Pool(processes=8) as pool:\n",
    "#     # loop over smaller chunks\n",
    "#     # for tinterval in tintervals[0:1]:\n",
    "#     tinterval = tintervals[0]\n",
    "#     print(tinterval)\n",
    "#     groups= spikes_df['GroupNumber'].loc[(spikes_df['Time'] >= tinterval.left) &  (spikes_df['Time'] < tinterval.right)].unique()\n",
    "#     print(len(groups))\n",
    "#     group_df_list = pool.map(process_group, groups)\n",
    "#     df = pd.concat(group_df_list)\n",
    "#     date = tinterval.left\n",
    "#     # Write to parquet file\n",
    "#     pdir = PurePath(outputdir, '{:d}/{:02d}'.format(date.year, date.month))\n",
    "#     os.makedirs(pdir)\n",
    "#     df_path = PurePath(pdir, 'df_coincidentals_{:d}_{:02d}_{:02d}.parquet'.format(date.year, date.month, date.day))\n",
    "#     df.to_parquet(df_file, engine='pyarrow', compression=None)\n",
    "#     print('wrote to parquet file: ', df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with Pool(processes=8) as pool:\n",
    "    tinterval = tintervals[0]\n",
    "    print(tinterval)\n",
    "    groups= spikes_df['GroupNumber'].loc[(spikes_df['Time'] >= '2010-12-01 00:00:00') &  (spikes_df['Time'] < '2010-12-01 23:59:59')].unique()\n",
    "    # groups= spikes_df['GroupNumber'].loc[(spikes_df['Time'] >= tinterval.left) &  (spikes_df['Time'] < tinterval.right)].unique()\n",
    "    print(len(groups))\n",
    "    group_df_list = pool.map(process_group, groups, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
