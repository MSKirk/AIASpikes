{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fitsio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coincidentals(spikes_list, idx):\n",
    "    \n",
    "    # Spikes coordinates at given wavelength index\n",
    "    spikes_w = spikes_list[idx]\n",
    "    # Associated neighbour coordinates\n",
    "    nb_pixels = index_8nb[spikes_w[0, :], :]\n",
    "    # Sublist of spikes data that will exclude the one serving as template\n",
    "    spikes_sublist = spikes_list[:idx]+spikes_list[idx+1:]\n",
    "    # Coincidental cross-referencing. \n",
    "    mask_w_arr = np.array([np.isin(nb_pixels, index_8nb[spikes[0,:], :]).any(axis=1) for spikes in spikes_sublist])\n",
    "    select_pixels = mask_w_arr.any(axis=0)\n",
    "    coords_w = spikes_w[0, select_pixels] \n",
    "    w_tables = np.insert(mask_w_arr[:, select_pixels], idx, True, axis=0)\n",
    "    # Retrieve intensity values for the selected coordinates\n",
    "    intensities = spikes_w[ 1:, select_pixels]\n",
    "    arr_w = np.concatenate([coords_w[np.newaxis,...], intensities, w_tables], axis=0)\n",
    "    arr_w = np.insert(arr_w, 3, idx, axis=0)\n",
    "    \n",
    "    return arr_w\n",
    "\n",
    "def extract_coincidentals2(spikes_list, idx):\n",
    "    \n",
    "    # Spikes coordinates at given wavelength index\n",
    "    spikes_w = spikes_list[idx]\n",
    "    # Associated neighbour coordinates\n",
    "    nb_pixels = index_8nb[spikes_w[0, :], :]\n",
    "    # Sublist of spikes data that will exclude the one serving as template\n",
    "    spikes_sublist = spikes_list[:idx]+spikes_list[idx+1:]\n",
    "    # Coincidental cross-referencing. \n",
    "    mask_w_arr = np.array([np.isin(nb_pixels, spikes[0,:]).any(axis=1) for spikes in spikes_sublist])\n",
    "    select_pixels = mask_w_arr.any(axis=0)\n",
    "    coords_w = spikes_w[0, select_pixels] \n",
    "    w_tables = np.insert(mask_w_arr[:, select_pixels], idx, True, axis=0)\n",
    "    # Retrieve intensity values for the selected coordinates\n",
    "    intensities = spikes_w[ 1:, select_pixels]\n",
    "    arr_w = np.concatenate([coords_w[np.newaxis,...], intensities, w_tables], axis=0)\n",
    "    arr_w = np.insert(arr_w, 3, idx, axis=0)\n",
    "    \n",
    "    return arr_w\n",
    "\n",
    "def process_group(fpaths, group_n):\n",
    "    spikes_list = [fitsio.read(os.path.join(data_dir, f)) for f in fpaths]\n",
    "    group_data = np.concatenate([extract_coincidentals(spikes_list, i) for i in range(7)], axis=1).T\n",
    "    column_names = ['coords' , 'int1', 'int2', 'wref', 'w0', 'w1', 'w2', 'w3', 'w4', 'w5', 'w6']\n",
    "    coincidental_spikes_df = pd.DataFrame(group_data, columns=column_names)\n",
    "    coincidental_spikes_df['GroupNumber'] = group_n\n",
    "    return coincidental_spikes_df\n",
    "\n",
    "\n",
    "def process_group2(fpaths, group_n):\n",
    "    spikes_list = [fitsio.read(os.path.join(data_dir, f)) for f in fpaths]\n",
    "    group_data = np.concatenate([extract_coincidentals2(spikes_list, i) for i in range(7)], axis=1).T\n",
    "    column_names = ['coords' , 'int1', 'int2', 'wref', 'w0', 'w1', 'w2', 'w3', 'w4', 'w5', 'w6']\n",
    "    coincidental_spikes_df = pd.DataFrame(group_data, columns=column_names)\n",
    "    coincidental_spikes_df['GroupNumber'] = group_n\n",
    "    return coincidental_spikes_df\n",
    "\n",
    "\n",
    "def filter_groups_intervals(groups_list):\n",
    "\n",
    "    empty1 = [i for i, groups in enumerate(groups_list) if not groups]\n",
    "    idx, groups_list2 = zip(*[(i, groups) for i, groups in enumerate(groups_list) if groups])\n",
    "    starts, ends = zip(*[[groups[0], groups[-1]] for groups in groups_list2])\n",
    "    overlaps = np.where(np.isin(starts, ends))[0]\n",
    "    for i in overlaps:\n",
    "        del groups_list2[i][0]\n",
    "    idx2, groups_list3 = zip(*[(i, groups) for i, groups in enumerate(groups_list2) if groups])\n",
    "    idx3 = [idx[i] for i in idx2]\n",
    "    \n",
    "    return groups_list3, idx3, empty1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16777216, 9)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################\n",
    "# Pre-compute the 8-connectivity lookup table. This will be shared across parallel workers.\n",
    "################################################################################################\n",
    "# List of relative 2D coordinates for 8-neighbour connectiviy (9-element list). 1st one is the origin pixel.\n",
    "coords_8nb = np.array([[0, 0], [-1, 0], [-1, -1], [0, -1], [1, -1], [1, 0], [1, 1], [0, 1], [-1, 1]])\n",
    "# Array of 2D coordinates for a 4096 x 4096 array. Matrix convention is kept. [rows, cols] = [y-axis, x-axis]\n",
    "ny, nx = [4096, 4096]\n",
    "coords_1d = np.arange(nx * ny)\n",
    "coordy, coordx = np.unravel_index(coords_1d, [ny, nx]) # also possible by raveling a meshgrid() output\n",
    "coords2d = np.array([coordy, coordx])\n",
    "# Create the array of 2D coordinates of 8-neighbours associated with each pixel.\n",
    "# pixel 0 has 8 neighbour + itself, pixel 1 has 8 neighbour + itself, etc...\n",
    "coords2d_8nb = coords2d[np.newaxis, ...] + coords_8nb[..., np.newaxis]\n",
    "# Handle off-edges coordinates by clipping to the edges, operation done in-place. Here, square detector assumed. Update\n",
    "# to per-axis clipping if that ever changes for another instrument.\n",
    "np.clip(coords2d_8nb, 0, nx-1, out=coords2d_8nb)\n",
    "# Convert to 1D coordinates.\n",
    "index_8nb = np.array([coords2d_8nb[i, 0, :] * nx + coords2d_8nb[i, 1, :] for i in range(len(coords_8nb))],\n",
    "                     dtype='int32', order='C').T\n",
    "index_8nb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = os.path.expanduser('~/Data/AIA_Spikes/SPIKESDF/parquet_dataframes2/2010/05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroupNumber  Time                            \n",
       "0            2010-05-13 00:00:02.090000+00:00    2010/05/13/2010-05-13T00:00:02.09Z_0193.spikes...\n",
       "             2010-05-13 00:00:03.570000+00:00    2010/05/13/2010-05-13T00:00:03.57Z_0094.spikes...\n",
       "             2010-05-13 00:00:05.070000+00:00    2010/05/13/2010-05-13T00:00:05.07Z_0335.spikes...\n",
       "             2010-05-13 00:00:06.580000+00:00    2010/05/13/2010-05-13T00:00:06.58Z_0171.spikes...\n",
       "             2010-05-13 00:00:08.080000+00:00    2010/05/13/2010-05-13T00:00:08.08Z_0211.spikes...\n",
       "Name: Path, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.environ['SPIKESDATA']\n",
    "spikes_df = pd.read_parquet(os.path.join(data_dir, 'spikes_df_2010.parquet'), engine='pyarrow')\n",
    "spikes_df2 = spikes_df.set_index(['GroupNumber', 'Time'])\n",
    "path_Series = spikes_df2['Path']\n",
    "path_Series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tintervals = [pd.Interval(left=pd.Timestamp('2010-05-13 00:00:00', tz='UTC'), right=pd.Timestamp('2010-05-13 01:00:00', tz='UTC')),\n",
    "                  pd.Interval(left=pd.Timestamp('2010-05-13 01:00:00', tz='UTC'), right=pd.Timestamp('2010-05-13 02:00:00', tz='UTC')),\n",
    "                  pd.Interval(left=pd.Timestamp('2010-05-13 02:00:00', tz='UTC'), right=pd.Timestamp('2010-05-13 03:00:00', tz='UTC'))]\n",
    "\n",
    "tinterval = tintervals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_ = [spikes_df['GroupNumber'].loc[(spikes_df['Time'] >= tinterval.left) & (spikes_df['Time'] < tinterval.right)].unique() for tinterval in tintervals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 299],\n",
       "       [300, 599],\n",
       "       [600, 899]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds = np.array([(groups[0], groups[-1]) for groups in groups_])\n",
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "tintervals = pd.interval_range(start=pd.Timestamp('2010-05-29 00:00:00', tz='UTC'),\n",
    "                                   end=pd.Timestamp('2010-12-30 00:00:00', tz='UTC'),\n",
    "                                   freq='1H', closed='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_ = [spikes_df['GroupNumber'].loc[(spikes_df['Time'] >= tinterval.left) & (spikes_df['Time'] < tinterval.right)].unique().tolist() for tinterval in tintervals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_2, tinds, empties = filter_groups_intervals(groups_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5160, 5160, 5121, 5121, 39)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tintervals), len(groups_), len(groups_2), len(tinds), len(empties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [5,]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "del a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, groups_list2 = zip(*[(idx, groups) for idx, groups in enumerate(groups_) if groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts, ends = zip(*[[groups[0], groups[-1]] for groups in groups_ if groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlaps = np.where(np.isin(starts, ends))[0]\n",
    "overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116100 116101\n",
      "116101 116102\n"
     ]
    }
   ],
   "source": [
    "group3 = groups_[3]\n",
    "print(group3[0], group3[1])\n",
    "del group3[0]\n",
    "print(group3[0], group3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2], [3, 4], [5, 6, 7]]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[0, 1, 2], [2, 3, 4], [5, 6, 7]]\n",
    "del a[1][0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = spikes_df['GroupNumber'].loc[(spikes_df['Time'] >= tinterval.left) & (spikes_df['Time'] < tinterval.right)].unique()\n",
    "paths_list_groups = [path_Series.loc[group_n] for group_n in groups]\n",
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "groupdf = process_group(paths_list_groups[0], groups[0])\n",
    "len(groupdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "groupdf2 = process_group2(paths_list_groups[0], groups[0])\n",
    "len(groupdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df_list = [process_group(paths_list, groups[i]) for i, paths_list in enumerate(paths_list_groups)]\n",
    "len(group_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(group_df_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df.loc[df['GroupNumber']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords0 = (df0['coords'].value_counts()[df0['coords'].value_counts() > 1]).index.values\n",
    "coords0[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['coords'] == coords0[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 4:11].sum(axis=1) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(df, npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.to_parquet(os.path.join(outputdir, 'temp.parquet'), engine='pyarrow', partition_on=['GroupNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf2 = dd.read_parquet(os.path.join(outputdir, 'temp.parquet'), engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=5, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_list_groups = [path_Series.loc[group_n] for group_n in groups[0:40]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_files = db.from_sequence(paths_list_groups, npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data_list = bag_files.map(process_group).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del group_data_list\n",
    "del bag_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
