{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fitsio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.environ['SPIKESDATA']\n",
    "spikes_db = pd.read_parquet(os.path.join(data_dir, 'spikes_df_2010.parquet'), engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Size</th>\n",
       "      <th>Wavelength</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GroupNumber</th>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">0</th>\n",
       "      <th>2010-05-13 00:00:02.090000+00:00</th>\n",
       "      <td>2010/05/13/2010-05-13T00:00:02.09Z_0193.spikes...</td>\n",
       "      <td>106560</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-13 00:00:03.570000+00:00</th>\n",
       "      <td>2010/05/13/2010-05-13T00:00:03.57Z_0094.spikes...</td>\n",
       "      <td>103680</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-13 00:00:05.070000+00:00</th>\n",
       "      <td>2010/05/13/2010-05-13T00:00:05.07Z_0335.spikes...</td>\n",
       "      <td>126720</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-13 00:00:06.580000+00:00</th>\n",
       "      <td>2010/05/13/2010-05-13T00:00:06.58Z_0171.spikes...</td>\n",
       "      <td>40320</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-13 00:00:08.080000+00:00</th>\n",
       "      <td>2010/05/13/2010-05-13T00:00:08.08Z_0211.spikes...</td>\n",
       "      <td>60480</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-13 00:00:09.580000+00:00</th>\n",
       "      <td>2010/05/13/2010-05-13T00:00:09.58Z_0304.spikes...</td>\n",
       "      <td>106560</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-13 00:00:11.080000+00:00</th>\n",
       "      <td>2010/05/13/2010-05-13T00:00:11.08Z_0131.spikes...</td>\n",
       "      <td>100800</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">1</th>\n",
       "      <th>2010-05-13 00:00:14.080000+00:00</th>\n",
       "      <td>2010/05/13/2010-05-13T00:00:14.08Z_0193.spikes...</td>\n",
       "      <td>43200</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-13 00:00:15.580000+00:00</th>\n",
       "      <td>2010/05/13/2010-05-13T00:00:15.58Z_0094.spikes...</td>\n",
       "      <td>100800</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-13 00:00:17.080000+00:00</th>\n",
       "      <td>2010/05/13/2010-05-13T00:00:17.08Z_0335.spikes...</td>\n",
       "      <td>126720</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-13 00:00:18.580000+00:00</th>\n",
       "      <td>2010/05/13/2010-05-13T00:00:18.58Z_0171.spikes...</td>\n",
       "      <td>40320</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-13 00:00:20.090000+00:00</th>\n",
       "      <td>2010/05/13/2010-05-13T00:00:20.09Z_0211.spikes...</td>\n",
       "      <td>60480</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-13 00:00:21.580000+00:00</th>\n",
       "      <td>2010/05/13/2010-05-13T00:00:21.58Z_0304.spikes...</td>\n",
       "      <td>106560</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-13 00:00:23.070000+00:00</th>\n",
       "      <td>2010/05/13/2010-05-13T00:00:23.07Z_0131.spikes...</td>\n",
       "      <td>100800</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           Path  \\\n",
       "GroupNumber Time                                                                                  \n",
       "0           2010-05-13 00:00:02.090000+00:00  2010/05/13/2010-05-13T00:00:02.09Z_0193.spikes...   \n",
       "            2010-05-13 00:00:03.570000+00:00  2010/05/13/2010-05-13T00:00:03.57Z_0094.spikes...   \n",
       "            2010-05-13 00:00:05.070000+00:00  2010/05/13/2010-05-13T00:00:05.07Z_0335.spikes...   \n",
       "            2010-05-13 00:00:06.580000+00:00  2010/05/13/2010-05-13T00:00:06.58Z_0171.spikes...   \n",
       "            2010-05-13 00:00:08.080000+00:00  2010/05/13/2010-05-13T00:00:08.08Z_0211.spikes...   \n",
       "            2010-05-13 00:00:09.580000+00:00  2010/05/13/2010-05-13T00:00:09.58Z_0304.spikes...   \n",
       "            2010-05-13 00:00:11.080000+00:00  2010/05/13/2010-05-13T00:00:11.08Z_0131.spikes...   \n",
       "1           2010-05-13 00:00:14.080000+00:00  2010/05/13/2010-05-13T00:00:14.08Z_0193.spikes...   \n",
       "            2010-05-13 00:00:15.580000+00:00  2010/05/13/2010-05-13T00:00:15.58Z_0094.spikes...   \n",
       "            2010-05-13 00:00:17.080000+00:00  2010/05/13/2010-05-13T00:00:17.08Z_0335.spikes...   \n",
       "            2010-05-13 00:00:18.580000+00:00  2010/05/13/2010-05-13T00:00:18.58Z_0171.spikes...   \n",
       "            2010-05-13 00:00:20.090000+00:00  2010/05/13/2010-05-13T00:00:20.09Z_0211.spikes...   \n",
       "            2010-05-13 00:00:21.580000+00:00  2010/05/13/2010-05-13T00:00:21.58Z_0304.spikes...   \n",
       "            2010-05-13 00:00:23.070000+00:00  2010/05/13/2010-05-13T00:00:23.07Z_0131.spikes...   \n",
       "\n",
       "                                                Size  Wavelength  \n",
       "GroupNumber Time                                                  \n",
       "0           2010-05-13 00:00:02.090000+00:00  106560         193  \n",
       "            2010-05-13 00:00:03.570000+00:00  103680          94  \n",
       "            2010-05-13 00:00:05.070000+00:00  126720         335  \n",
       "            2010-05-13 00:00:06.580000+00:00   40320         171  \n",
       "            2010-05-13 00:00:08.080000+00:00   60480         211  \n",
       "            2010-05-13 00:00:09.580000+00:00  106560         304  \n",
       "            2010-05-13 00:00:11.080000+00:00  100800         131  \n",
       "1           2010-05-13 00:00:14.080000+00:00   43200         193  \n",
       "            2010-05-13 00:00:15.580000+00:00  100800          94  \n",
       "            2010-05-13 00:00:17.080000+00:00  126720         335  \n",
       "            2010-05-13 00:00:18.580000+00:00   40320         171  \n",
       "            2010-05-13 00:00:20.090000+00:00   60480         211  \n",
       "            2010-05-13 00:00:21.580000+00:00  106560         304  \n",
       "            2010-05-13 00:00:23.070000+00:00  100800         131  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spikes_db2 = spikes_db.set_index(['GroupNumber', 'Time'])\n",
    "spikes_db2.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the filepaths (typically 7) for a given group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2010/05/13/2010-05-13T00:00:02.09Z_0193.spikes.fits',\n",
       "       '2010/05/13/2010-05-13T00:00:03.57Z_0094.spikes.fits',\n",
       "       '2010/05/13/2010-05-13T00:00:05.07Z_0335.spikes.fits',\n",
       "       '2010/05/13/2010-05-13T00:00:06.58Z_0171.spikes.fits',\n",
       "       '2010/05/13/2010-05-13T00:00:08.08Z_0211.spikes.fits',\n",
       "       '2010/05/13/2010-05-13T00:00:09.58Z_0304.spikes.fits',\n",
       "       '2010/05/13/2010-05-13T00:00:11.08Z_0131.spikes.fits'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spikes_db2.loc[0]['Path'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437 µs ± 2.64 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fpaths = spikes_db2.loc[0]['Path'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_filepaths(group_nb, file_paths, unique_indices, group_count, data_directory):\n",
    "    \"\"\" Get the path of each file belonging to the given group number\n",
    "\n",
    "    :param group_nb: group number\n",
    "    :param file_paths: numpy array of all relative file paths\n",
    "    :param unique_indices: indices of the unique group number\n",
    "    :param group_count: how many files in that group\n",
    "    :param data_directory: directory to append relative paths to make them absolute\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the path of each file of the given group number (group_index).\n",
    "    path_index = unique_indices[group_nb]\n",
    "    # Get how many files in the group (should typically be 7, for 7 wavelengths)\n",
    "    count = group_count[group_nb]\n",
    "    paths = [os.path.join(data_directory, fpath) for fpath in file_paths[path_index:path_index + count]]\n",
    "    return paths\n",
    "\n",
    "\n",
    "def delete_files(folder):\n",
    "    for filename in os.listdir(folder):\n",
    "        os.remove(os.path.abspath(os.path.join(folder, filename)))\n",
    "\n",
    "\n",
    "def filter_spike_file_rename(n_co_spikes, old_filename, output_dir):\n",
    "    # Return a modified filename of the filtered spike files.\n",
    "    basename = os.path.basename(old_filename)\n",
    "    new_name = 'filtered' + str(n_co_spikes)\n",
    "    return os.path.join(output_dir, basename.replace('spikes', new_name))\n",
    "\n",
    "\n",
    "def count_intersect(widx, raw_spikes, coincidental_1d_coords, count_filter_idx, counts):\n",
    "    \"\"\" Provides the coincidental coordinates and their indices in the raw spike file and occurence count\n",
    "    within the group. The indices in the raw spike file are used to retrieve the intensity values (before/after)\n",
    "\n",
    "    :param raw_spikes: list of spikes for one wavelength\n",
    "    :param coincidental_1d_coords: list of 1D coordinates of coincidental spikes integrated for the whole group\n",
    "    :param count_filter_idx: list of indices of the coincidental spikes mapping to the original list of spikes coords.\n",
    "    :param counts: distribution of spikes coords\n",
    "    :return: Coincidental coordinates, index in spike file, number of occurences >=n_co_spikes\n",
    "    \"\"\"\n",
    "\n",
    "    file_coords, idx1, idx2 = np.intersect1d(raw_spikes[0, :], coincidental_1d_coords, return_indices=True)\n",
    "    # Get intensity values at the coincidental coordinates\n",
    "    int_before = raw_spikes[1, idx1]\n",
    "    int_after = raw_spikes[2, idx1]\n",
    "    # Retrieve how many coincidental hits we had within the 8 neighbours.\n",
    "    group_counts = counts[count_filter_idx[idx2]]\n",
    "    # Map of the wavelength index, instead of actual wavelength value as 7-element group is 12s-time-based, not wavelength-based\n",
    "    widx = [widx]*len(file_coords)\n",
    "    \n",
    "    return file_coords, idx1, group_counts, widx, int_before, int_after\n",
    "\n",
    "\n",
    "def breakdown_coincidentals(spikes_list, coincidental_1d_coords, count_filter_idx, counts):\n",
    "    \"\"\" Provides the coincidental coordinates and their indices in the raw spike file and occurence count\n",
    "    within the group. The indices in the raw spike file are used to retrieve the intensity values (before/after)\n",
    "\n",
    "    :param raw_spikes: list of spikes for one wavelength\n",
    "    :param coincidental_1d_coords: list of 1D coordinates of coincidental spikes integrated for the whole group\n",
    "    :param count_filter_idx: list of indices of the coincidental spikes mapping to the original list of spikes coords.\n",
    "    :param counts: distribution of spikes coords\n",
    "    :return: Coincidental coordinates, index in spike file, number of occurences >=n_co_spikes\n",
    "    \"\"\"\n",
    "    \n",
    "    data_dict = {'coords':[], 'int1': [], 'int2': [], 'counts': [], 'widx': []}\n",
    "    \n",
    "    for widx, raw_spikes in enumerate(spikes_list):\n",
    "        file_coords, idx1, idx2 = np.intersect1d(raw_spikes[0, :], coincidental_1d_coords, return_indices=True)\n",
    "        # Get intensity values at the coincidental coordinates\n",
    "        #data = [raw_spikes[1, idx1], raw_spikes[2, idx1], counts[count_filter_idx[idx2]], [widx]*len(file_coords)]\n",
    "        # Map of the wavelength index, instead of actual wavelength value as 7-element group is 12s-time-based, not wavelength-based\n",
    "        data_dict['coords'].extend(file_coords)\n",
    "        data_dict['int1'].extend(raw_spikes[1, idx1])\n",
    "        data_dict['int2'].extend(raw_spikes[2, idx1])\n",
    "        data_dict['counts'].extend(counts[count_filter_idx[idx2]])\n",
    "        data_dict['widx'].extend([widx]*len(file_coords))\n",
    "        \n",
    "    \n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "npgroups = spikes_db.get('GroupNumber').values\n",
    "nppaths = spikes_db.get('Path').values\n",
    "# Filter the unique values of groups (ugroups), and output associated indices (uinds) and counts for each group (ugroupc)\n",
    "ugroups, uinds, ugroupc = np.unique(npgroups, return_index=True, return_counts=True)\n",
    "\n",
    "################################################################################################\n",
    "# Pre-compute the 8-connectivity lookup table. This will be shared across parallel workers.\n",
    "################################################################################################\n",
    "# List of relative 2D coordinates for 8-neighbour connectiviy (9-element list). 1st one is the origin pixel.\n",
    "coords_8nb = np.array([[0, 0], [-1, 0], [-1, -1], [0, -1], [1, -1], [1, 0], [1, 1], [0, 1], [-1, 1]])\n",
    "# Array of 2D coordinates for a 4096 x 4096 array. Matrix convention is kept. [rows, cols] = [y-axis, x-axis]\n",
    "ny, nx = [4096, 4096]\n",
    "coords_1d = np.arange(nx * ny)\n",
    "coordy, coordx = np.unravel_index(coords_1d, [ny, nx]) # also possible by raveling a meshgrid() output\n",
    "coords2d = np.array([coordy, coordx])\n",
    "# Create the array of 2D coordinates of 8-neighbours associated with each pixel.\n",
    "# pixel 0 has 8 neighbour + itself, pixel 1 has 8 neighbour + itself, etc...\n",
    "coords2d_8nb = coords2d[np.newaxis, ...] + coords_8nb[..., np.newaxis]\n",
    "# Handle off-edges coordinates by clipping to the edges, operation done in-place. Here, square detector assumed. Update\n",
    "# to per-axis clipping if that ever changes for another instrument.\n",
    "np.clip(coords2d_8nb, 0, nx-1, out=coords2d_8nb)\n",
    "# Convert to 1D coordinates.\n",
    "index_8nb = np.array([coords2d_8nb[i, 0, :] * nx + coords2d_8nb[i, 1, :] for i in range(len(coords_8nb))],\n",
    "                     dtype='int32', order='C')\n",
    "\n",
    "n_co_spikes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.09 ms ± 89.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# for group_n in ugroups:\n",
    "group_n = 0\n",
    "# timeit -> year 2010: 8 us  8 years:\n",
    "fpaths = get_filepaths(group_n, nppaths, uinds, ugroupc, data_dir)\n",
    "spikes_list = [fitsio.read(path) for path in fpaths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb of coincidental coordinates =  19928\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# accumulat_spikes()\n",
    "\n",
    "# spikes list: [7 files] x [1D coordinates, intensity before despiking replacement, intensity after despiking]\n",
    "cumulated_spikes_coords = np.unique(index_8nb[:, spikes_list[0][0, :]].ravel())\n",
    "for raw_spikes in spikes_list[1:]:\n",
    "    # Accumulate the coordinates across the 7 files into a single 1D array.\n",
    "    cumulated_spikes_coords = np.concatenate([cumulated_spikes_coords, np.unique(index_8nb[:, raw_spikes[0, :]].ravel())])\n",
    "# Make a curated distribution (numbers that do not exist aren't covered by the algorithm => faster than histogram)\n",
    "(distrib_values, counts) = np.unique(cumulated_spikes_coords, return_counts=True) # 35 ms\n",
    "# Get the indices of the coordinates that get hit more than n_co_spikes times\n",
    "count_filter_idx = np.where(counts >= n_co_spikes)[0]\n",
    "# Get these coincicental spikes coordinates\n",
    "coincidental_1d_coords = distrib_values[count_filter_idx] # 1 ms\n",
    "print('nb of coincidental coordinates = ', len(coincidental_1d_coords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'coords':[], 'int1': [], 'int2': [], 'counts': [], 'widx': []}\n",
    "nbours = np.unique(index_8nb[:, spikes_list[0][0, :]].ravel())\n",
    "# Get nearest neighbours of the raw spikes at each group and find those that are coincidentals\n",
    "w_coords = np.intersect1d(nbours, coincidental_1d_coords, assume_unique=True)\n",
    "# We need the 0-nb coordinates to get back to to the intensities\n",
    "#spikes_list[0]\n",
    "\n",
    "# d['coords'].extend(file_coords)\n",
    "# d['widx'].extend([0]*len(file_coords))\n",
    "# d['counts'].extend(counts[count_filter_idx[idx2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.98 ms ± 7.48 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit file_coords = np.intersect1d(nbours, coincidental_1d_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 30356)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = index_8nb[:, spikes_list[1][0, :]]\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6852"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = breakdown_coincidentals(spikes_list, coincidental_1d_coords, count_filter_idx, counts)\n",
    "len(data_dict['coords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_coords, idx1, idx2 = np.intersect1d(raw_spikes[0, :], coincidental_1d_coords, return_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
